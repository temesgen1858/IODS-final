---
title: "IODS Final Project"
author: "Abera Temesgen"
email: "temesgen.abera@helsinki.fi"
date: "15 December 2017"


output:
  html_document:
    code_folding: hide
    fig_caption: yes
    fig_height: 4
    fig_width: 6
    theme: journal
    toc: yes
    toc_depth: 2
---
***
#Abstract
In this assignment, it is attempted to determine whether there is a linear relationship between an independent variable(crime rate) and three explanatory variables(index of accessibility to radial highways, percentage of lower status of population, and property tax rate) utilizing readily availabe Boston data in r MASS package. Three statistical methods(Linear regression, logistic regression and Principal Component analysis) were used to carryout the analsyis. The regression result indicated that crime rate has linear, positive and statistically significant relation with index of accessibility to radial highways and percentage of lower status of population, with an r square of 0.42. Moreover, compared to the percentage of lower status of population, crime rate variation is more strongly explained by index of accessibility to radial highways in Boston. On the other hand,PCA analysis showed that the first and the second principal components reflects 46.8% and 11.8% of the total variations in the original boston data. 

***

***
#Introduction

Welcome to my final assignment page. In this exercise, we will investigate whether there is a meaningful relationship between per capita crime rate in 506-towns in Boston(USA), and  percentage of lower status of the population in the towns and  full-value property-tax rate. The data source will be from the Boston data in the r MASS package. 
 
Related to the research problem, three hypothesis are stated as follows:

**Hypothesis 1: Increase in the index of accessibility to radial highways in towns favors high crime rate**

**Hypothesis 2: Crime rates increase with high percentage of lower status of population** 

**Hypothesis 3: Crime rates inrease in rich towns as indicated by their high property tax rate** 

To test these hypothesis, I will use two statistical methods:linear and logistic regression. Linear regression is a statistical method to identify the extent to which there is a linear relationship between a dependent variable and one or more independent variabels. The independent variables are used to predict the values of the dependent(response) variable. The dependent variables must be measured on a continious measurement sale(i.g. 0-100) while the independent variable(s) can be measured on either a continous or categorical measurement scale. In order to qualify for linear regression, the data must satisfy some key assumptions:(1) there must be a linear relationship between the dependent variable and the independent variables, (2) independent variables are not highly correlated with each other, and (3) the variance of error terms are similar across the values of the independent variables.

Logistics regressions is a special type of regression where binary response variable is related to a set of explanatory variables, which can be discrete and/or continuous.In logistic regression Probability or Odds of the response taking a particular value is modeled based on combination of values taken by the predictors.Logistic regression is applicable if we want to: classify individuals into two categories based on explanatory variables, predict probabilities that individuals fall into two categories of the binary response as a function of some explanatory variables, perform descriptive discriminate analyses such as describing the differences between individuals in separate groups as a function of explanatory variables, etc.
   
Moreover, to reduce the dimensionality of the original Boston data, principal component anlysis (PCA) will be used at later section (see PCA). PCA is a dimension-reduction tool that can be used to reduce a large set of variables to a small set that still contains most of the information in the large set by transforming  a number of correlated variables into a smaller number of uncorrelated variables called principal components.The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible.



***

```{r child = "Chapter1.Rmd"}

```

***